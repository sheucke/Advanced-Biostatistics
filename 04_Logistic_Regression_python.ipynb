{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_Logistic_Regression_python.ipynb","provenance":[],"authorship_tag":"ABX9TyMXno9zQoeN/LVreuFnMfEj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ra3zTCRiqX9m"},"source":["## Logistic Regression from Scratch"]},{"cell_type":"markdown","metadata":{"id":"Kt9ojcl9qqKL"},"source":["Logistic Regression is used when the dependent variable(target) is categorical.\n","\n","For example,\n","\n","* To predict whether an email is spam (1) or (0)\n","\n","* Whether the tumor is malignant (1) or not (0)\n","\n","![logist](https://docs.google.com/uc?export=download&id=1efGPMyw2MmRZIZlHTkjtOWsoBe_A1vMt)\n","\n","Consider a scenario where we need to classify whether an email is spam or not. If we use linear regression for this problem, there is a need for setting up a threshold based on which classification can be done. Say if the actual class is malignant, predicted continuous value 0.4 and the threshold value is 0.5, the data point will be classified as not malignant which can lead to serious consequence in real time.\n","From this example, it can be inferred that linear regression is not suitable for classification problem. Linear regression is unbounded, and this brings logistic regression into picture. Their value strictly ranges from 0 to 1.\n","\n","**Simple Logistic Regression**\n","\n","**Model**\n","\n","Output = 0 or 1\n","\n","Hypothesis => Z = WX + B\n","\n","hΘ(x) = sigmoid (Z)\n","\n","**Sigmoid Function**\n","\n","![sigm](https://docs.google.com/uc?export=download&id=1u-voV-t6WGgy5CzaD43doOpHyvkii8Sq)\n","\n","If ‘Z’ goes to infinity, Y(predicted) will become 1 and if ‘Z’ goes to negative infinity, Y(predicted) will become 0.\n","\n","Data is fit into linear regression model, which then be acted upon by a logistic function predicting the target categorical dependent variable.\n","\n","**Types of Logistic Regression**\n","\n","1. Binary Logistic Regression\n","\n","The categorical response has only two 2 possible outcomes. Example: Spam or Not\n","\n","2. Multinomial Logistic Regression\n","\n","Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan)\n","\n","3. Ordinal Logistic Regression\n","\n","Three or more categories with ordering. Example: Movie rating from 1 to 5\n","\n","**Decision Boundary**\n","\n","To predict which class a data belongs, a threshold can be set. Based upon this threshold, the obtained estimated probability is classified into classes.\n","\n","Say, if predicted_value ≥ 0.5, then classify email as spam else as not spam.\n","\n","Decision boundary can be linear or non-linear. Polynomial order can be increased to get complex decision boundary."]},{"cell_type":"code","metadata":{"id":"X4UcggOCqr9N","executionInfo":{"status":"ok","timestamp":1604651258808,"user_tz":-60,"elapsed":970,"user":{"displayName":"sebastian heucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgU7r1bq5bIRNqaoCYdLPidtmcPCdoxFiu0JStl=s64","userId":"03230349622000046876"}}},"source":["import numpy as np\n","\n","class LogisticRegression:\n","\n","    def __init__(self, learning_rate=0.001, n_iters=1000):\n","        self.lr = learning_rate\n","        self.n_iters = n_iters\n","        self.weights = None\n","        self.bias = None\n","\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","\n","        # init parameters\n","        self.weights = np.zeros(n_features)\n","        self.bias = 0\n","\n","        # gradient descent\n","        for _ in range(self.n_iters):\n","            # approximate y with linear combination of weights and x, plus bias\n","            linear_model = np.dot(X, self.weights) + self.bias\n","            # apply sigmoid function\n","            y_predicted = self._sigmoid(linear_model)\n","\n","            # compute gradients\n","            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n","            db = (1 / n_samples) * np.sum(y_predicted - y)\n","            # update parameters\n","            self.weights -= self.lr * dw\n","            self.bias -= self.lr * db\n","\n","    def predict(self, X):\n","        linear_model = np.dot(X, self.weights) + self.bias\n","        y_predicted = self._sigmoid(linear_model)\n","        y_predicted_cls = [1 if i > 0.5 else 0 for i in y_predicted]\n","        return np.array(y_predicted_cls)\n","\n","    def _sigmoid(self, x):\n","        return 1 / (1 + np.exp(-x))"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"hoqh-blwqvzI","executionInfo":{"status":"ok","timestamp":1604651264332,"user_tz":-60,"elapsed":1507,"user":{"displayName":"sebastian heucke","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgU7r1bq5bIRNqaoCYdLPidtmcPCdoxFiu0JStl=s64","userId":"03230349622000046876"}},"outputId":"ff007844-1617-410f-eb88-c5ed13214023","colab":{"base_uri":"https://localhost:8080/"}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","import matplotlib.pyplot as plt\n","\n","def accuracy(y_true, y_pred):\n","    accuracy = np.sum(y_true == y_pred) / len(y_true)\n","    return accuracy\n","\n","bc = datasets.load_breast_cancer()\n","X, y = bc.data, bc.target\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n","\n","regressor = LogisticRegression(learning_rate=0.0001, n_iters=1000)\n","regressor.fit(X_train, y_train)\n","predictions = regressor.predict(X_test)\n","\n","print(\"LR classification accuracy:\", accuracy(y_test, predictions))"],"execution_count":2,"outputs":[{"output_type":"stream","text":["LR classification accuracy: 0.9298245614035088\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IAAvNXxOqyM7"},"source":[""],"execution_count":null,"outputs":[]}]}